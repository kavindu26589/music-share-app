const peer = new Peer();
let call;
let mixedStream;
let volumeMonitor;

const statusEl = document.getElementById("status");
const debugLog = document.getElementById("debug-log");

peer.on("open", id => {
  document.getElementById("my-id").textContent = id;
  logDebug(`ðŸ†” PeerJS ID: ${id}`);
});

peer.on("call", incomingCall => {
  incomingCall.answer();
  incomingCall.on("stream", stream => {
    const audio = new Audio();
    audio.srcObject = stream;
    audio.autoplay = true;
    statusEl.textContent = "ðŸ“¡ Receiving stream from caller.";

    stream.getAudioTracks().forEach((track, i) => {
      logDebug(`ðŸ“¥ Received Track ${i + 1}: ${track.label}`);
    });
  });
});

async function startCall() {
  const peerId = document.getElementById("peer-id").value;
  statusEl.textContent = "ðŸ”„ Preparing stream...";
  logDebug("ðŸŽ§ Attempting to capture system audio + mic...");

  try {
    let displayStream = null;
    try {
      displayStream = await navigator.mediaDevices.getDisplayMedia({
        video: true, // to allow tab selection
        audio: true
      });
    } catch (err) {
      logDebug("âš ï¸ System audio capture not supported or denied.");
    }

    const micStream = await navigator.mediaDevices.getUserMedia({ audio: true });

    const audioContext = new AudioContext();
    const destination = audioContext.createMediaStreamDestination();

    if (displayStream) {
      const sysSource = audioContext.createMediaStreamSource(displayStream);
      sysSource.connect(destination);
      monitorVolume(sysSource, audioContext);
      displayStream.getAudioTracks().forEach((t, i) =>
        logDebug(`ðŸ”Š System Audio Track ${i + 1}: ${t.label}`)
      );
    } else {
      logDebug("âŒ No system audio, mic only.");
    }

    const micSource = audioContext.createMediaStreamSource(micStream);
    micSource.connect(destination);
    micStream.getAudioTracks().forEach((t, i) =>
      logDebug(`ðŸŽ¤ Mic Track ${i + 1}: ${t.label}`)
    );

    mixedStream = destination.stream;

    call = peer.call(peerId, mixedStream);
    call.on("stream", stream => {
      const audio = new Audio();
      audio.srcObject = stream;
      audio.autoplay = true;
      statusEl.textContent = "âœ… Connected and streaming.";
    });

    statusEl.textContent = "ðŸ“ž Calling...";
  } catch (err) {
    alert("âŒ Error: " + err.message);
    logDebug("âŒ Error during startCall: " + err.message);
    statusEl.textContent = "âŒ Failed to start.";
  }
}

function stopCall() {
  if (call) {
    call.close();
    call = null;
    statusEl.textContent = "ðŸ”Œ Call ended.";
    logDebug("ðŸ›‘ Call manually stopped.");
  }

  if (mixedStream) {
    mixedStream.getTracks().forEach(track => track.stop());
  }

  if (volumeMonitor) {
    cancelAnimationFrame(volumeMonitor);
  }
}

function logDebug(msg) {
  debugLog.textContent += `\n${msg}`;
  debugLog.scrollTop = debugLog.scrollHeight;
}

function monitorVolume(source, audioContext) {
  const analyser = audioContext.createAnalyser();
  analyser.fftSize = 512;
  const bufferLength = analyser.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);

  source.connect(analyser);

  function updateVolume() {
    analyser.getByteFrequencyData(dataArray);
    const avg = dataArray.reduce((a, b) => a + b) / bufferLength;
    const emoji = avg > 10 ? "ðŸŽµ Music Detected" : "ðŸ”‡ Silence";
    statusEl.textContent = `ðŸ“¶ Volume Level: ${Math.floor(avg)} | ${emoji}`;
    volumeMonitor = requestAnimationFrame(updateVolume);
  }

  updateVolume();
}
